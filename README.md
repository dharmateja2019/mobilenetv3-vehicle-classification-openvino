Input Image
   â†“
Vehicle Detection (SSD-MobileNet)
   â†“
Crop Vehicle(s)
   â†“
Vehicle Classification (PyTorch / OpenVINO)
   â†“
Vehicle Color Detection (HSV)
   â†“
Optional VLM Reasoning
     â”œâ”€ Vehicle-level reasoning
     â””â”€ Full-image fallback (if detection fails)

ğŸš€ Features

SSD-MobileNet vehicle detection (COCO)

MobileNetV3 classification (ImageNet)

Classical CV color detection (HSV)

Refined center-crop color extraction

PyTorch FP32 vs OpenVINO FP16 comparison

Per-backend latency & FPS metrics

CLI for automation & benchmarking

Streamlit GUI with Run button

Real Visionâ€“Language Model (VLM) integration

Clean compare mode (PyTorch vs OpenVINO)

ğŸ›  Requirements

Python 3.11

OS: Linux / macOS / Windows

CPU (GPU / FPGA selectable in GUI, optional)

ğŸ§ª Environment Setup
1ï¸âƒ£ Create virtual environment
python3 -m venv mobilenet_env
source mobilenet_env/bin/activate

2ï¸âƒ£ Install dependencies
pip install --upgrade pip
pip install torch torchvision
pip install opencv-python
pip install openvino
pip install streamlit pandas numpy
pip install openai

ğŸ”‘ VLM Setup (Required for VLM)

This project uses a real Visionâ€“Language Model (OpenAI GPT-4o-mini).

Set your API key:

export OPENAI_API_KEY="your_api_key_here"


âš ï¸ VLM is optional and disabled by default.
It is not used for FPS benchmarking.

ğŸ“¦ Model Downloads (VERY IMPORTANT)
ğŸ”¹ A. SSD-MobileNet (Vehicle Detection)
mkdir -p models/ssd_mobilenet
cd models/ssd_mobilenet

wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz
tar -xvf ssd_mobilenet_v2_coco_2018_03_29.tar.gz

mv ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb .
wget https://raw.githubusercontent.com/opencv/opencv_extra/master/testdata/dnn/ssd_mobilenet_v2_coco_2018_03_29.pbtxt


Final structure:

models/ssd_mobilenet/
â”œâ”€â”€ frozen_inference_graph.pb
â””â”€â”€ ssd_mobilenet_v2_coco_2018_03_29.pbtxt

ğŸ”¹ B. MobileNetV3 OpenVINO Model (Classification)

Place converted OpenVINO IR files here:

models/ir/
â”œâ”€â”€ mobilenetv3.xml
â””â”€â”€ mobilenetv3.bin


âš ï¸ Conversion scripts are intentionally excluded.

ğŸ“ Project Structure
project_root/
â”œâ”€â”€ app.py
â”œâ”€â”€ README.md
â”œâ”€â”€ outputs/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ ssd_mobilenet/
â”‚   â””â”€â”€ ir/
â””â”€â”€ src/
    â”œâ”€â”€ run_infer_cli.py
    â”œâ”€â”€ detector_ssd_mobilenet.py
    â”œâ”€â”€ inference_baseline.py
    â”œâ”€â”€ inference_openvino.py
    â”œâ”€â”€ color_extractor.py
    â”œâ”€â”€ preprocess.py
    â”œâ”€â”€ postprocess.py
    â”œâ”€â”€ metrics.py
    â””â”€â”€ vlm_reasoner.py

ğŸ§ª CLI Usage
â–¶ Run with OpenVINO
python src/run_infer_cli.py --image path/to/image.jpg --backend openvino

â–¶ Run with PyTorch
python src/run_infer_cli.py --image path/to/image.jpg --backend pytorch

â–¶ Compare PyTorch vs OpenVINO
python src/run_infer_cli.py --image path/to/image.jpg --backend compare

â–¶ Enable VLM reasoning
python src/run_infer_cli.py --image path/to/image.jpg --backend openvino --use-vlm


CLI Output Includes:

   Per-backend latency & FPS

   Per-vehicle type & color

   Optional VLM reasoning

   VLM fallback when detection fails

ğŸ–¥ Streamlit GUI
streamlit run app.py

GUI Features

   Upload image

   Select backend (PyTorch / OpenVINO / Compare)

   Select device (CPU / GPU / AUTO / FPGA â€“ UI level)

   Run Inference button

   View bounding boxes

   Per-vehicle type & color

   Per-backend latency & FPS

   Optional VLM reasoning section

   Annotated output image

âš ï¸ Known Limitations (Current Stage)

   SSD-MobileNet may misclassify:

   Front-facing cars

   Scooters in narrow lanes

   ImageNet classifier is not vehicle-specific

   VLM adds latency and should not be used for benchmarking

   These are model limitations, not pipeline bugs.

ğŸ”® Planned Enhancements

   YOLOv8 detector option

   Better 2-wheeler recall

   VLM override policy (trust VLM over CV)

   Video & multi-object tracking

   GPU / FPGA enablement in OpenVINO

   CSV export of benchmark results

ğŸ‘¨â€ğŸ’» Engineering Notes

This project emphasizes system design over raw accuracy.

It demonstrates:

   Multi-backend inference

   Honest benchmarking

   Explainable AI via VLM

   Clean CLI & GUI separation

   Real-world AI orchestration patterns